# Haoyang Huang (黄浩洋)

[Google Scholar](https://scholar.google.com/citations?user=nIS66toAAAAJ&hl=en) \| [LinkedIn](https://www.linkedin.com/in/%E6%B5%A9%E6%B4%8B-%E9%BB%84-77a59016a/)

### <span style="color: #ff0000;">We are currently seeking highly motivated researchers and interns to join us for unified multimodal models: huanghaoyang.ocean@jd.com (work) or hhy338189@gmail.com (personal).</span>

Huang Haoyang is head of the Multimodal Foundation Models team at JD Discovery Academy, leading research on unified multimodal models. He holds a master's from Peking University, won top domestic data mining awards, and is a Kaggle Master. At Microsoft Research Asia, he focused on multimodal and multilingual foundation models, developing the multilingual model [Unicoder](https://arxiv.org/abs/1909.00964) (covering 100 languages) and leading the world’s first multilingual multimodal pre-trained model [M3P](https://arxiv.org/abs/2006.02635). He has published dozens of papers in CVPR, ACL, EMNLP, AAAI, with 2k+ Google Scholar citations. In 2024, he joined StepFun, leading and open-sourcing the 30B StepVideo series ([Step-Video-T2V](https://arxiv.org/abs/2502.10248), [Step-Video-TI2V](https://arxiv.org/abs/2503.11251)).

黄浩洋现任京东探索学院图像与多模态实验室多模态基础模型团队负责人，带领团队开展统一多模态模型研究。他拥有北京大学硕士学位，曾获得国内顶级数据挖掘奖项，并获得 Kaggle Master 称号。在微软亚洲研究院，他专注于多模态与多语言基础模型，研发了覆盖 100 种语言的多语言模型 Unicoder，并主导了全球首个多语言多模态预训练模型 M3P 的研发。他在 CVPR、ACL、EMNLP、AAAI 等顶级会议发表数十篇论文，谷歌学术引用量超过 2,000 次。2024 年，他加入 StepFun，主导并开源了 30B StepVideo 模型系列（Step-Video-T2V、Step-Video-TI2V）。


# Highlights

###  <ul style="text-align: left;">
  <li><strong>
    Multimodal Foundation Model: </strong> <a href="https://arxiv.org/abs/2006.02635" style="color:blue;">M3P</a> (CVPR, 2021), <a href="https://arxiv.org/abs/2002.06353" style="color:blue;">UNIVL</a> (Preprint, 2021), <a href="https://aclanthology.org/2021.acl-long.156/" style="color:blue;">HCN</a> (ACL, 2021), <a href="https://arxiv.org/abs/2202.05009" style="color:blue;">NUWA-LIP</a> (CVPR, 2023), <a href="https://arxiv.org/abs/2502.10248" style="color:blue;">Step-Video-T2V</a> (Technical Report, 2025), <a href="https://arxiv.org/abs/2503.11251" style="color:blue;">Step-Video-TI2V</a> (Technical Report, 2025)
  </li>  
  <li><strong>Multilingual Foundation Model: </strong> <a href="https://arxiv.org/abs/1909.00964" style="color:blue;">Unicoder</a> (EMNLP, 2019), <a href="https://arxiv.org/abs/2305.07004" style="color:blue;">XLT</a> (EMNLP, 2023), <a href="https://aclanthology.org/2024.naacl-long.367/" style="color:blue;">Div-Ref</a> (NAACL, 2023), <a href="https://aclanthology.org/2024.emnlp-main.55/" style="color:blue;">CoD</a> (EMNLP, 2023), <a href="https://arxiv.org/abs/2402.13064" style="color:blue;">GLAN</a>(EMNLP, 2024), <a href="https://arxiv.org/abs/2402.16438" style="color:blue;">LAPE</a>(ACL, 2024)
  </li> 
  <li><strong>Machine Translation:</strong> <a href="https://arxiv.org/abs/2012.15547" style="color:blue;">XLM-T</a> (Preprint, 2020), <a href="https://aclanthology.org/2021.wmt-1.54" style="color:blue;">WMT21 First-Place Report</a> (SIGMT, 2021), <a href="https://arxiv.org/abs/2103.11878" style="color:blue;">BlonDe</a> (NAACL 2022), <a href="https://aclanthology.org/2022.emnlp-main.184/" style="color:blue;">LVP-M3</a> (EMNLP, 2022)
  </li> 
  </ul>

# Publication


###  <ul style="text-align: left;">
  <li>
    <strong>Step-Video-Ti2v Technical Teport</strong><br>
    Haoyang Huang, Guoqing Ma, Nan Duan, Xing Chen, Changyi Wan, Ranchen Ming, Tianyu Wang, Bo Wang, Zhiying Lu.<br> 
    Technical Report, 2025.
  </li>

  <li>
    <strong>Step-Video-T2v Technical Teport</strong><br>
    Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan.<br>
    Technical Report, 2025.
  </li>

  <li>
    <strong>STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives</strong><br>
    Bo Wang, Haoyang Huang, Zhiying Lu, Fengyuan Liu, Guoqing Ma, Jianlong Yuan, Yuan Zhang, Nan Duan, Daxin Jiang.<br>
    Preprint, 2025.
  </li>

  <li>
    <strong>Generative pre-trained autoregressive diffusion transformer</strong><br>
    Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, Nan Duan.<br>
    Preprint, 2025.
  </li>

  <li>
    <strong>Respond in my language: Mitigating language inconsistency in response generation based on large language models</strong><br>
    Liang Zhang, Qin Jin, Haoyang Huang, Dongdong Zhang, Furu Wei.<br>
    ACL, 2024.
  </li>

  <li>
    <strong>Language-specific neurons: The key to multilingual capabilities in large language models</strong><br>
    Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen.<br> 
    ACL, 2024.
  </li>

</ul>
